{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import pandas as pd\n", "import re\n", "from textblob import TextBlob\n", "from nltk.tokenize import word_tokenize\n", "from nltk.stem import WordNetLemmatizer\n", "from nltk.corpus import stopwords\n", "stop_words = set(stopwords.words('english'))\n", "from sklearn.feature_extraction.text import TfidfVectorizer\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.metrics import accuracy_score\n", "import warnings\n", "a=warnings.filterwarnings('ignore')\n", "import pickle\n", "from sklearn.svm import LinearSVC"]}, {"cell_type": "markdown", "metadata": {}, "source": ["tring Cleansing"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def data_processing(text):\n", "    text = text.lower()\n", "    text = re.sub(r\"https\\S+|www\\S+https\\S+\", '',text, flags=re.MULTILINE)\n", "    text = re.sub(r'\\@w+|\\#','',text)\n", "    text = re.sub(r'[^\\w\\s]','',text)\n", "    text_tokens = word_tokenize(text)\n", "    filtered_text = [w for w in text_tokens if not w in stop_words]\n", "    return \" \".join(filtered_text)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["emmatization"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def lemmatizing_frame(data):\n", "    lempred = WordNetLemmatizer()\n", "    l = len(data)\n", "    for i in range(0,l,1):\n", "        txt = []\n", "        list = data[i].split(\" \")\n", "    \n", "    \n", "        for w in list:\n", "            a = lempred.lemmatize(w)\n", "            txt.append(a)\n", "            textprocessed = ' '.join(map(str,txt))\n", "        data[i] = textprocessed"]}, {"cell_type": "markdown", "metadata": {}, "source": ["olarity"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def polarity(text):\n", "    return TextBlob(text).sentiment.polarity"]}, {"cell_type": "markdown", "metadata": {}, "source": ["entiment Labelling"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def sentiment(label):\n", "    if label <0:\n", "        return \"Negative\"\n", "    elif label ==0:\n", "        return \"Neutral\"\n", "    elif label>0:\n", "        return \"Positive\""]}, {"cell_type": "markdown", "metadata": {}, "source": ["xporting processed csv to train the model<br>\n", "ataset Import"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def ProcessedData(Dataset):\n", "    df = pd.read_csv(Dataset)\n\n", "    #Dropping columns\n", "    text_df = df.drop(['id', 'user_name', 'user_location', 'user_description', 'user_created',\n", "           'user_followers', 'user_friends', 'user_favourites', 'user_verified',\n", "           'date', 'hashtags', 'source', 'retweets', 'favorites',\n", "           'is_retweet'], axis=1)\n", "    text_df.text = text_df['text'].apply(data_processing)\n", "    lemmatizing_frame(text_df['text'])\n", "    text_df['polarity'] = text_df['text'].apply(polarity)\n", "    text_df['sentiment'] = text_df['polarity'].apply(sentiment)\n", "    text_df = text_df.drop_duplicates('text')\n", "    text_df = text_df.drop(['polarity'],axis=1)\n", "    return text_df.to_csv(r\"C:\\Users\\krupe\\OneDrive\\Desktop\\Sentiment\\.csv\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["ProcessedData(r\"C:\\Users\\krupe\\OneDrive\\Desktop\\Sentiment\\vaccination_all_tweets.csv\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df = pd.read_csv(r\"C:\\Users\\krupe\\OneDrive\\Desktop\\Sentiment\\.csv\")\n", "vectoriser = TfidfVectorizer(ngram_range=(1,2)).fit(df['text'])\n", "vector = \"vectoriser.sav\"\n", "pickle.dump(vectoriser, open(vector,'wb'))\n", "X = df['text']\n", "Y = df['sentiment']\n", "X = vectoriser.transform(X)\n", "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["SVCmodel = LinearSVC()\n", "SVCmodel.fit(x_train, y_train)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["filesvc = 'finalized_model_SVC.sav'\n", "pickle.dump(SVCmodel, open(filesvc, 'wb'))"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}